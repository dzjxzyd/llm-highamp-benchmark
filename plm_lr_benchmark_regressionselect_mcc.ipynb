{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b51205e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, re, json, argparse, warnings\n",
    "from typing import List, Dict, Optional\n",
    "import time\n",
    "from sklearn.model_selection import StratifiedKFold, GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "30c4af29",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc93d005",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel, AutoConfig, T5EncoderModel\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, balanced_accuracy_score, roc_auc_score,\n",
    "    matthews_corrcoef, confusion_matrix, average_precision_score, make_scorer\n",
    ")\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b688cb3b",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def safe_model_tag(model_id: str) -> str:\n",
    "    return re.sub(r\"[^a-zA-Z0-9_.-]+\", \"_\", model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "701a39f7",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def clean_aa(seq: str) -> str:\n",
    "    # keep only standard AA-ish; map rare to X\n",
    "    return seq.strip().upper().replace(\"U\",\"X\").replace(\"Z\",\"X\").replace(\"O\",\"X\").replace(\"B\",\"X\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "340ea292",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def needs_space_separated(model_id: str) -> bool:\n",
    "    mid = model_id.lower()\n",
    "    return any(k in mid for k in [\"rostlab\", \"prot_t5\", \"prot_bert\", \"distilprotbert\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9c35ae5d",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def mean_pool(last_hidden: torch.Tensor,\n",
    "              attn_mask: torch.Tensor,\n",
    "              special_mask: Optional[torch.Tensor] = None) -> torch.Tensor:\n",
    "    mask = attn_mask.bool()\n",
    "\n",
    "    # special_mask 有些 tokenizer 可能行为怪（甚至全 1），要做“保底”\n",
    "    if special_mask is not None:\n",
    "        keep = mask & (~special_mask.bool())\n",
    "        # 如果某些样本被 special_mask 全剔空，就忽略 special_mask（否则 pooled 会全 0）\n",
    "        bad = (keep.sum(dim=1) == 0)\n",
    "        if bad.any():\n",
    "            keep[bad] = mask[bad]\n",
    "        mask = keep\n",
    "\n",
    "    mask_f = mask.unsqueeze(-1).float()            # [B, L, 1]\n",
    "    summed = (last_hidden * mask_f).sum(dim=1)     # [B, H]\n",
    "    denom = mask_f.sum(dim=1).clamp(min=1.0)       # [B, 1]\n",
    "    return summed / denom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "80592281",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def compute_metrics(y_true: np.ndarray, y_prob: np.ndarray, y_pred: np.ndarray) -> Dict[str, float]:\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    bacc = balanced_accuracy_score(y_true, y_pred)\n",
    "    mcc = matthews_corrcoef(y_true, y_pred)\n",
    "\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred, labels=[0, 1]).ravel()\n",
    "    sn = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
    "    sp = tn / (tn + fp) if (tn + fp) > 0 else 0.0\n",
    "\n",
    "    try:\n",
    "        auc = roc_auc_score(y_true, y_prob)\n",
    "    except ValueError:\n",
    "        auc = float(\"nan\")\n",
    "\n",
    "    ap = average_precision_score(y_true, y_prob)\n",
    "    return {\"ACC\": acc, \"BACC\": bacc, \"Sn\": sn, \"Sp\": sp, \"MCC\": mcc, \"AUC\": auc, \"AP\": ap}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "63f99739",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def load_model_tokenizer_transformers(model_id: str, device: str):\n",
    "    tok = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)\n",
    "    cfg = AutoConfig.from_pretrained(model_id, trust_remote_code=True)\n",
    "\n",
    "    is_ankh = model_id.lower().startswith(\"synthyra/ankh\")\n",
    "\n",
    "    if is_ankh:\n",
    "        torch_dtype = torch.float32   # ✅ ANKH 强制 FP32\n",
    "        model = T5EncoderModel.from_pretrained(model_id, torch_dtype=torch_dtype)\n",
    "    else:\n",
    "        torch_dtype = torch.float16 if str(device).startswith(\"cuda\") else torch.float32\n",
    "        if getattr(cfg, \"model_type\", \"\") == \"t5\":\n",
    "            model = T5EncoderModel.from_pretrained(model_id, torch_dtype=torch_dtype)\n",
    "        else:\n",
    "            model = AutoModel.from_pretrained(model_id, trust_remote_code=True, torch_dtype=torch_dtype)\n",
    "\n",
    "    model.to(device).eval()\n",
    "    return tok, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e588acdc",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def embed_sequences_esmc(model_id: str, seqs: List[str], device: str) -> np.ndarray:\n",
    "    if \"300m\" in model_id.lower():\n",
    "        esm_name = \"esmc_300m\"\n",
    "    elif \"600m\" in model_id.lower():\n",
    "        esm_name = \"esmc_600m\"\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown ESMC size in model_id: {model_id}\")\n",
    "\n",
    "    from esm.models.esmc import ESMC\n",
    "    from esm.sdk.api import ESMProtein, LogitsConfig\n",
    "\n",
    "    client = ESMC.from_pretrained(esm_name).to(device)\n",
    "    client.eval()\n",
    "\n",
    "    vecs = []\n",
    "    with torch.no_grad():\n",
    "        for seq in tqdm(seqs, desc=f\"Embedding (ESMC:{esm_name})\"):\n",
    "            s = clean_aa(seq)\n",
    "            protein = ESMProtein(sequence=s)\n",
    "            protein_tensor = client.encode(protein)\n",
    "            out = client.logits(protein_tensor, LogitsConfig(sequence=True, return_embeddings=True))\n",
    "\n",
    "            emb = out.embeddings\n",
    "            seq_emb = getattr(emb, \"sequence\", None)\n",
    "            if seq_emb is None and isinstance(emb, dict):\n",
    "                seq_emb = emb.get(\"sequence\", None)\n",
    "            if seq_emb is None:\n",
    "                seq_emb = emb\n",
    "\n",
    "            if not torch.is_tensor(seq_emb):\n",
    "                seq_emb = torch.tensor(seq_emb)\n",
    "\n",
    "            if seq_emb.dim() == 3:\n",
    "                seq_emb = seq_emb[0]  # [L, D]\n",
    "\n",
    "            # 常见：len(seq)+2 含 BOS/EOS\n",
    "            if seq_emb.size(0) == len(s) + 2:\n",
    "                seq_emb = seq_emb[1:-1]\n",
    "\n",
    "            vec = seq_emb.mean(dim=0)\n",
    "            vecs.append(vec.float().cpu().numpy())\n",
    "\n",
    "    X = np.stack(vecs, axis=0)\n",
    "    return np.nan_to_num(X, nan=0.0, posinf=0.0, neginf=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2fa135d8",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def embed_sequences_esm3(seqs: List[str], device: str) -> np.ndarray:\n",
    "    from esm.models.esm3 import ESM3\n",
    "    from esm.sdk.api import ESMProtein, SamplingConfig\n",
    "\n",
    "    client = ESM3.from_pretrained(\"esm3_sm_open_v1\").to(device)\n",
    "    client.eval()\n",
    "\n",
    "    vecs = []\n",
    "    with torch.no_grad():\n",
    "        for seq in tqdm(seqs, desc=\"Embedding (ESM3:esm3_sm_open_v1)\"):\n",
    "            s = clean_aa(seq)\n",
    "            protein = ESMProtein(sequence=s)\n",
    "            protein_tensor = client.encode(protein)\n",
    "\n",
    "            out = client.forward_and_sample(\n",
    "                protein_tensor,\n",
    "                SamplingConfig(return_per_residue_embeddings=True)\n",
    "            )\n",
    "\n",
    "            emb = getattr(out, \"per_residue_embedding\", None)\n",
    "            if emb is None:\n",
    "                emb = getattr(out, \"per_residue_embeddings\", None)\n",
    "            if emb is None:\n",
    "                raise RuntimeError(\"ESM3 output has no per_residue_embedding(s).\")\n",
    "\n",
    "            if emb.dim() == 3:\n",
    "                emb = emb[0]  # [L, D]\n",
    "\n",
    "            if emb.size(0) == len(s) + 2:\n",
    "                emb = emb[1:-1]\n",
    "\n",
    "            vec = emb.mean(dim=0)\n",
    "            vecs.append(vec.float().cpu().numpy())\n",
    "\n",
    "    X = np.stack(vecs, axis=0)\n",
    "    return np.nan_to_num(X, nan=0.0, posinf=0.0, neginf=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4b5cc5a4",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def embed_sequences_transformers(model_id: str,\n",
    "                                seqs: List[str],\n",
    "                                device: str,\n",
    "                                batch_size: int,\n",
    "                                max_length: int) -> np.ndarray:\n",
    "    tok, model = load_model_tokenizer_transformers(model_id, device)\n",
    "\n",
    "    all_vecs = []\n",
    "    for i in tqdm(range(0, len(seqs), batch_size), desc=f\"Embedding {model_id}\"):\n",
    "        batch_seqs = [clean_aa(s) for s in seqs[i:i+batch_size]]\n",
    "        batch_text = [\" \".join(list(s)) for s in batch_seqs] if needs_space_separated(model_id) else batch_seqs\n",
    "\n",
    "        enc = tok(\n",
    "            batch_text,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=max_length,\n",
    "            return_special_tokens_mask=True,\n",
    "        )\n",
    "        # tokenizer-side quick checks (on CPU ok)\n",
    "        input_ids_cpu = enc[\"input_ids\"]\n",
    "        unk_id = getattr(tok, \"unk_token_id\", None)\n",
    "        if unk_id is not None:\n",
    "            unk_rate = (input_ids_cpu == unk_id).float().mean().item()\n",
    "            if unk_rate > 0.2:\n",
    "                print(f\"[WARN] high UNK rate={unk_rate:.3f} for {model_id}\")\n",
    "\n",
    "        special_cpu = enc.get(\"special_tokens_mask\", None)\n",
    "        if special_cpu is not None:\n",
    "            all_special = (special_cpu.sum(dim=1) == special_cpu.size(1)).any().item()\n",
    "            if all_special:\n",
    "                print(f\"[WARN] special_tokens_mask marks ALL tokens as special for {model_id} (will auto-fallback).\")\n",
    "\n",
    "        enc = {k: v.to(device) for k, v in enc.items()}\n",
    "        attn_mask = enc.get(\"attention_mask\", None)\n",
    "        special_mask = enc.get(\"special_tokens_mask\", None)\n",
    "\n",
    "        out = model(**{k: enc[k] for k in [\"input_ids\", \"attention_mask\"] if k in enc})\n",
    "        last_hidden = out.last_hidden_state\n",
    "\n",
    "        if attn_mask is None:\n",
    "            attn_mask = torch.ones(last_hidden.shape[:2], device=device, dtype=torch.long)\n",
    "\n",
    "        is_t5_like = (\"ankh\" in model_id.lower()) or (getattr(model, \"config\", None) and getattr(model.config, \"model_type\", \"\") == \"t5\")\n",
    "\n",
    "        if is_t5_like:\n",
    "            vec = mean_pool(last_hidden, attn_mask, special_mask=None)   # ✅ T5/ANKH：别传 special_mask\n",
    "        else:\n",
    "            vec = mean_pool(last_hidden, attn_mask, special_mask)\n",
    "        \"\"\"\n",
    "        if (vec.abs().sum(dim=1) == 0).any().item():\n",
    "            print(f\"[WARN] zero pooled embedding exists for {model_id} (check tokenizer/pooling)\")\n",
    "        \n",
    "        if i <= 3:\n",
    "            print(\"Example input:\", batch_text[0][:80])\n",
    "            print(\"input_ids[0][:30]:\", enc[\"input_ids\"][0][:30].tolist())\n",
    "            print(\"attention_mask sum[0]:\", int(enc[\"attention_mask\"][0].sum()))\n",
    "            if \"input_ids\" in enc and enc[\"input_ids\"].size(0) > 1:\n",
    "                print(\"input_ids[1][:30]:\", enc[\"input_ids\"][1][:30].tolist())\n",
    "        \"\"\"\n",
    "        all_vecs.append(vec.detach().cpu().float().numpy())\n",
    "\n",
    "    X = np.concatenate(all_vecs, axis=0)\n",
    "    return np.nan_to_num(X, nan=0.0, posinf=0.0, neginf=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "34513168",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def embed_sequences_dplm(model_id: str, seqs: List[str], device: str) -> np.ndarray:\n",
    "    try:\n",
    "        from byprot.models.lm.dplm import DiffusionProteinLanguageModel  # type: ignore\n",
    "        dplm = DiffusionProteinLanguageModel.from_pretrained(model_id)\n",
    "        dplm.to(device).eval()\n",
    "\n",
    "        # NOTE: byprot API varies across versions. We'll try a robust path.\n",
    "        vecs = []\n",
    "        with torch.no_grad():\n",
    "            for seq in tqdm(seqs, desc=f\"Embedding (DPLM:{model_id})\"):\n",
    "                s = clean_aa(seq)\n",
    "                # common: dplm.encode / dplm.get_representation not guaranteed\n",
    "                if hasattr(dplm, \"encode\"):\n",
    "                    rep = dplm.encode([s])\n",
    "                elif hasattr(dplm, \"get_representation\"):\n",
    "                    rep = dplm.get_representation([s])\n",
    "                else:\n",
    "                    # last resort: forward with tokenizer inside model (may fail)\n",
    "                    rep = dplm([s])\n",
    "\n",
    "                rep = torch.as_tensor(rep)\n",
    "                rep = rep.squeeze(0)  # [L,D] or [D]\n",
    "                if rep.dim() == 2:\n",
    "                    rep = rep.mean(dim=0)\n",
    "                vecs.append(rep.float().detach().cpu().numpy())\n",
    "\n",
    "        X = np.stack(vecs, axis=0)\n",
    "        return np.nan_to_num(X, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "\n",
    "    except Exception as e:\n",
    "        warnings.warn(f\"[DPLM] byprot path failed ({type(e).__name__}: {e}). Fallback to transformers.\")\n",
    "        return embed_sequences_transformers(model_id, seqs, device, batch_size=2, max_length=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "996aeef4",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def embed_sequences(model_id: str,\n",
    "                    seqs: List[str],\n",
    "                    device: str,\n",
    "                    batch_size: int,\n",
    "                    cache_dir: str,\n",
    "                    max_length: int) -> np.ndarray:\n",
    "    os.makedirs(cache_dir, exist_ok=True)\n",
    "    tag = safe_model_tag(model_id)\n",
    "    cache_path = os.path.join(cache_dir, f\"{tag}.npy\")\n",
    "\n",
    "    if os.path.exists(cache_path):\n",
    "        return np.load(cache_path)\n",
    "\n",
    "    # ---- model-specific routing ----\n",
    "    if model_id.startswith(\"EvolutionaryScale/esmc-\"):\n",
    "        X = embed_sequences_esmc(model_id, seqs, device)\n",
    "\n",
    "    elif model_id == \"EvolutionaryScale/esm3-sm-open-v1\":\n",
    "        X = embed_sequences_esm3(seqs, device)\n",
    "\n",
    "    elif model_id == \"airkingbd/dplm_650m\":\n",
    "        X = embed_sequences_dplm(model_id, seqs, device)\n",
    "\n",
    "    elif model_id.startswith(\"westlake-repl/SaProt_\"):\n",
    "        # SaProt 通常需要 seq+structure tokens；只有 AA seq 时结果可能接近基线。\n",
    "        print(\"[NOTE] SaProt usually expects structure-aware tokens (often containing '#'). \"\n",
    "              \"If you only have AA sequences, performance may collapse to majority baseline.\")\n",
    "        X = embed_sequences_transformers(model_id, seqs, device, batch_size, max_length)\n",
    "\n",
    "    else:\n",
    "        # ESM2 / ANKH / Mistral-Prot\n",
    "        X = embed_sequences_transformers(model_id, seqs, device, batch_size, max_length)\n",
    "\n",
    "    np.save(cache_path, X)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2183c074",
   "metadata": {},
   "outputs": [],
   "source": [
    "def solver_cv_select_strict_mcc(X_tr, y_tr, seed=42, out_csv=\"solver_cv_table.csv\", n_jobs=8):\n",
    "    \"\"\"STRICT model selection:\n",
    "    - Use ONLY training data (X_tr/y_tr) to compare solvers and pick hyperparams.\n",
    "    - Selection metric: MCC (via CV).\n",
    "    - Returns: (best_estimator_fitted_on_Xtr, cv_table_df, best_info_dict)\n",
    "    \"\"\"\n",
    "    base_pipe = Pipeline(steps=[\n",
    "        (\"imp\", SimpleImputer(strategy=\"constant\", fill_value=0.0)),\n",
    "        (\"scaler\", MinMaxScaler()),\n",
    "        (\"clf\", LogisticRegression(\n",
    "            max_iter=5000,\n",
    "            class_weight=\"balanced\",\n",
    "            random_state=seed\n",
    "        ))\n",
    "    ])\n",
    "\n",
    "    scoring = {\n",
    "        \"mcc\": make_scorer(matthews_corrcoef),\n",
    "        \"auc\": \"roc_auc\",\n",
    "        \"bacc\": \"balanced_accuracy\",\n",
    "    }\n",
    "\n",
    "    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=seed)\n",
    "\n",
    "    solver_grids = {\n",
    "        \"lbfgs\": [\n",
    "            {\"clf__solver\": [\"lbfgs\"], \"clf__penalty\": [\"l2\"], \"clf__C\": [0.1, 1, 10]}\n",
    "        ],\n",
    "        \"liblinear\": [\n",
    "            {\"clf__solver\": [\"liblinear\"], \"clf__penalty\": [\"l1\", \"l2\"], \"clf__C\": [0.1, 1, 10]}\n",
    "        ],\n",
    "        \"newton-cg\": [\n",
    "            {\"clf__solver\": [\"newton-cg\"], \"clf__penalty\": [\"l2\"], \"clf__C\": [0.1, 1, 10]}\n",
    "        ],\n",
    "        \"saga\": [\n",
    "            {\"clf__solver\": [\"saga\"], \"clf__penalty\": [\"l1\", \"l2\"], \"clf__C\": [0.1, 1, 10]},\n",
    "            {\"clf__solver\": [\"saga\"], \"clf__penalty\": [\"elasticnet\"], \"clf__C\": [0.1, 1, 10], \"clf__l1_ratio\": [0.2, 0.5, 0.8]},\n",
    "        ],\n",
    "    }\n",
    "\n",
    "    rows = []\n",
    "    best_overall = None\n",
    "    best_info = {\"best_solver_group\": None, \"best_params\": None, \"cv_best_mcc\": -1e18}\n",
    "\n",
    "    for solver_name, grid in solver_grids.items():\n",
    "        t0 = time.time()\n",
    "\n",
    "        gs = GridSearchCV(\n",
    "            estimator=base_pipe,\n",
    "            param_grid=grid,\n",
    "            scoring=scoring,\n",
    "            refit=\"mcc\",      # ★ STRICT: select by MCC\n",
    "            cv=cv,\n",
    "            n_jobs=n_jobs,\n",
    "            verbose=0,\n",
    "            return_train_score=False\n",
    "        )\n",
    "        gs.fit(X_tr, y_tr)\n",
    "\n",
    "        best_mcc = float(gs.best_score_)\n",
    "        # For reference (same hyperparams picked by MCC)\n",
    "        best_idx = gs.best_index_\n",
    "        best_auc = float(gs.cv_results_[\"mean_test_auc\"][best_idx]) if \"mean_test_auc\" in gs.cv_results_ else float(\"nan\")\n",
    "        best_bacc = float(gs.cv_results_[\"mean_test_bacc\"][best_idx]) if \"mean_test_bacc\" in gs.cv_results_ else float(\"nan\")\n",
    "\n",
    "        dt = time.time() - t0\n",
    "        row = {\n",
    "            \"solver_group\": solver_name,\n",
    "            \"cv_best_mcc\": best_mcc,\n",
    "            \"cv_auc_at_best_mcc\": best_auc,\n",
    "            \"cv_bacc_at_best_mcc\": best_bacc,\n",
    "            **{f\"best_{k}\": v for k, v in gs.best_params_.items()},\n",
    "            \"seconds\": dt,\n",
    "        }\n",
    "        rows.append(row)\n",
    "        print(f\"[CV:{solver_name}] best_mcc={best_mcc:.4f} best_params={gs.best_params_} time={dt:.1f}s\")\n",
    "\n",
    "        if best_mcc > best_info[\"cv_best_mcc\"]:\n",
    "            best_info[\"cv_best_mcc\"] = best_mcc\n",
    "            best_info[\"best_solver_group\"] = solver_name\n",
    "            best_info[\"best_params\"] = gs.best_params_\n",
    "            best_overall = gs.best_estimator_\n",
    "\n",
    "    df_out = pd.DataFrame(rows).sort_values(\"cv_best_mcc\", ascending=False)\n",
    "    df_out.to_csv(out_csv, index=False)\n",
    "    print(\"Saved CV table:\", out_csv)\n",
    "    print(\"Selected (by MCC):\", best_info[\"best_solver_group\"], \"cv_best_mcc=\", best_info[\"cv_best_mcc\"])\n",
    "    return best_overall, df_out, best_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "db6a757a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "\n",
    "def build_parser():\n",
    "    ap = argparse.ArgumentParser(prog=\"plm_lr_benchmark\")  # prog 可选；别传 argv list\n",
    "    ap.add_argument(\"--data_csv\", required=True)\n",
    "    ap.add_argument(\"--seq_col\", default=\"sequence\")\n",
    "    ap.add_argument(\"--label_col\", default=\"label\")\n",
    "    ap.add_argument(\"--model_id\", required=True)\n",
    "    ap.add_argument(\"--device\", default=\"cuda\")\n",
    "    ap.add_argument(\"--batch_size\", type=int, default=8)\n",
    "    ap.add_argument(\"--max_length\", type=int, default=256)\n",
    "    ap.add_argument(\"--test_size\", type=float, default=0.2)\n",
    "    ap.add_argument(\"--seed\", type=int, default=42)\n",
    "    ap.add_argument(\"--cache_dir\", default=\"cache\")\n",
    "    ap.add_argument(\"--n_jobs\", type=int, default=8)\n",
    "    ap.add_argument(\"--out_dir\", default=\"results\")\n",
    "    return ap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f726a3a9",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def main(argv=None):\n",
    "    ap = build_parser()\n",
    "    args = ap.parse_args(argv)\n",
    "    print(args)\n",
    "\n",
    "    os.makedirs(args.out_dir, exist_ok=True)\n",
    "\n",
    "    df = pd.read_csv(args.data_csv)\n",
    "    if args.seq_col not in df.columns:\n",
    "        raise KeyError(f\"seq_col '{args.seq_col}' not found. Columns={list(df.columns)}\")\n",
    "    if args.label_col not in df.columns:\n",
    "        raise KeyError(f\"label_col '{args.label_col}' not found. Columns={list(df.columns)}\")\n",
    "\n",
    "    seqs = df[args.seq_col].astype(str).tolist()\n",
    "    y = df[args.label_col].astype(int).to_numpy()\n",
    "\n",
    "    X = embed_sequences(args.model_id, seqs, args.device, args.batch_size, args.cache_dir, args.max_length)\n",
    "\n",
    "    print(\"X shape:\", X.shape)\n",
    "    print(\"X mean std over dims:\", X.std(axis=0).mean())\n",
    "    print(\"Unique rows (rounded):\", np.unique(X.round(6), axis=0).shape[0], \"/\", X.shape[0])\n",
    "\n",
    "    # 1) One fixed split (test is held-out and used ONCE)\n",
    "    idx = np.arange(len(y))\n",
    "    tr_idx, te_idx = train_test_split(\n",
    "        idx,\n",
    "        test_size=args.test_size,\n",
    "        random_state=args.seed,\n",
    "        shuffle=True,\n",
    "        stratify=y\n",
    "    )\n",
    "\n",
    "    split_path = os.path.join(args.out_dir, f\"split_seed{args.seed}_test{args.test_size}.npz\")\n",
    "    np.savez(split_path, train_idx=tr_idx, test_idx=te_idx)\n",
    "    print(\"Saved split:\", split_path)\n",
    "\n",
    "    X_tr, X_te = X[tr_idx], X[te_idx]\n",
    "    y_tr, y_te = y[tr_idx], y[te_idx]\n",
    "\n",
    "    # 2) STRICT model selection (ONLY on training data) using MCC\n",
    "    tag = safe_model_tag(args.model_id)\n",
    "    cv_csv = os.path.join(args.out_dir, f\"solver_cv_{tag}_seed{args.seed}_test{args.test_size}.csv\")\n",
    "\n",
    "    best_pipe, df_cv, best_info = solver_cv_select_strict_mcc(\n",
    "        X_tr, y_tr,\n",
    "        seed=args.seed,\n",
    "        out_csv=cv_csv,\n",
    "        n_jobs=getattr(args, \"n_jobs\", 8)\n",
    "    )\n",
    "\n",
    "    # 3) Final evaluation on held-out test (used ONCE)\n",
    "    y_prob = best_pipe.predict_proba(X_te)[:, 1]\n",
    "    y_pred = best_pipe.predict(X_te)  # consistent with MCC scorer (uses predict)\n",
    "    print(\"y positive rate:\", y.mean())\n",
    "    print(\"pred positive rate:\", y_pred.mean(), \"prob std:\", y_prob.std())\n",
    "\n",
    "    m = compute_metrics(y_te, y_prob, y_pred)\n",
    "\n",
    "    out = {\n",
    "        \"model_id\": args.model_id,\n",
    "        \"n\": int(len(df)),\n",
    "        \"dim\": int(X.shape[1]),\n",
    "        \"selection\": {\n",
    "            \"metric\": \"MCC\",\n",
    "            \"cv_table_csv\": os.path.basename(cv_csv),\n",
    "            \"best_solver_group\": best_info[\"best_solver_group\"],\n",
    "            \"best_params\": best_info[\"best_params\"],\n",
    "            \"cv_best_mcc\": float(best_info[\"cv_best_mcc\"]),\n",
    "        },\n",
    "        \"test_metrics\": m,\n",
    "        \"split\": {\"method\": \"train_test_split\", \"test_size\": args.test_size, \"seed\": args.seed},\n",
    "    }\n",
    "\n",
    "    out_path = os.path.join(args.out_dir, f\"{tag}.json\")\n",
    "    with open(out_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(out, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    print(\"Saved:\", out_path, flush=True)\n",
    "    print(json.dumps(out[\"selection\"], ensure_ascii=False, indent=2), flush=True)\n",
    "    print(json.dumps(m, ensure_ascii=False, indent=2), flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "576c4d31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(data_csv='data.csv', seq_col='sequence', label_col='label', model_id='EvolutionaryScale/esmc-300m-2024-12', device='cuda', batch_size=8, max_length=256, test_size=0.2, seed=42, cache_dir='cache', out_dir='results')\n",
      "X shape: (3444, 960)\n",
      "X mean std over dims: 0.014620587\n",
      "Unique rows (rounded): 3444 / 3444\n",
      "Saved split: results\\split_seed42_test0.2.npz\n",
      "Fitting 5 folds for each of 24 candidates, totalling 120 fits\n",
      "y positive rate: 0.6173054587688734\n",
      "pred positive rate: 0.5703918722786647 prob std: 0.28252706\n",
      "Saved: results\\EvolutionaryScale_esmc-300m-2024-12.json\n",
      "{\n",
      "  \"ACC\": 0.7474600870827286,\n",
      "  \"BACC\": 0.7443538324420678,\n",
      "  \"Sn\": 0.7576470588235295,\n",
      "  \"Sp\": 0.7310606060606061,\n",
      "  \"MCC\": 0.4799582357794938,\n",
      "  \"AUC\": 0.8236096256684493,\n",
      "  \"AP\": 0.8776901289796043\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "main([\n",
    "    \"--data_csv\",\"data.csv\",\n",
    "    \"--seq_col\",\"sequence\",\n",
    "    \"--label_col\",\"label\",\n",
    "    \"--model_id\",\"EvolutionaryScale/esmc-300m-2024-12\",\n",
    "    \"--batch_size\",\"8\",\n",
    "    \"--max_length\",\"256\",\n",
    "])"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
